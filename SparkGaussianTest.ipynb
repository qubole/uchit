{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.config.config_set import UniversalConfigSet\n",
    "from spark.model.gaussian_model import GaussianModel\n",
    "from spark.model.training_data import TrainingData\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingData()\n",
    "config_set = UniversalConfigSet(10, 1024 * 10)\n",
    "model = GaussianModel(config_set, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample_1 = {\n",
    "    \"spark.executor.memory\": 1024 * 5,\n",
    "    \"spark.sql.shuffle.partitions\": 100,\n",
    "    \"spark.executor.cores\": 4,\n",
    "    \"spark.driver.memory\": 1024\n",
    "}\n",
    "training_sample_2 = {\n",
    "    \"spark.executor.memory\": 1024 * 10,\n",
    "    \"spark.sql.shuffle.partitions\": 400,\n",
    "    \"spark.executor.cores\": 8,\n",
    "    \"spark.driver.memory\": 1024 * 3\n",
    "}\n",
    "model.add_sample_to_train_data(training_sample_1, 12)\n",
    "model.add_sample_to_train_data(training_sample_2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.96131309]\n",
      " [0.96131309 1.        ]]\n",
      "{'spark.executor.memory': 5017.0, 'spark.driver.memory': 768.0, 'spark.executor.cores': 7.0, 'spark.sql.shuffle.partitions': 60.0}\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "print model.get_training_pairwise_correlation()\n",
    "print model.get_best_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.96131309]\n",
      " [0.96131309 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print model.get_training_pairwise_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_values = model.get_sampled_configs()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8461538461538461, 0.1875, -0.0, -0.0]\n",
      "[[1.e-06]\n",
      " [1.e-06]\n",
      " [1.e-06]\n",
      " [1.e-06]]\n",
      "term1 [1.03365385e-06]\n",
      "term2 [[11.99999819]\n",
      " [ 3.99999425]]\n",
      "term3 [[0.93987784 0.03812624]]\n",
      "term4 [[11.43103713]]\n",
      "[[11.43103816]]\n"
     ]
    }
   ],
   "source": [
    "# DEBUG Paragraph\n",
    "# Mean Calculation\n",
    "config = model.get_sampled_configs()[0]\n",
    "print config\n",
    "print model.beta\n",
    "term1 = np.dot(config, model.beta)\n",
    "print \"term1 %s\" % term1\n",
    "term2 = model.training_out - np.dot(model.get_training_params(), model.beta)\n",
    "print \"term2 %s\" % term2\n",
    "term31 = model.get_correlation_with_train_data(config).transpose()\n",
    "term32 = np.linalg.inv(model.get_training_pairwise_correlation())\n",
    "term3 = np.dot(term31, term32)\n",
    "print \"term3 %s\" % term3\n",
    "term4 = np.dot(term3, term2)\n",
    "print \"term4 %s\" % term4\n",
    "print term1 + term4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9765291 ]\n",
      " [0.94164311]]\n",
      "[[1.         0.96131309]\n",
      " [0.96131309 1.        ]]\n",
      "[[0.04628063]]\n",
      "0.046280627946319715\n"
     ]
    }
   ],
   "source": [
    "# DEBUG Paragraph\n",
    "# Variance calculation\n",
    "config = model.get_sampled_configs()[0]\n",
    "corr_with_train_data = model.get_correlation_with_train_data(config)\n",
    "corr_pairwise_train_data = model.get_training_pairwise_correlation()\n",
    "print corr_with_train_data\n",
    "print corr_pairwise_train_data\n",
    "term1 = np.dot(corr_with_train_data.transpose(), np.linalg.inv(corr_pairwise_train_data))\n",
    "term2 = np.dot(term1, corr_with_train_data)\n",
    "term3 = 1 - term2\n",
    "print term3\n",
    "print np.linalg.det(term3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.62752883e-205]\n",
      "[0.02564102564102564, 0.5, 0.6666666666666666, 0.625]\n",
      "60.0\n",
      "1131.0\n",
      "276.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#DEBUG Paragraph\n",
    "import sys\n",
    "from spark.discretizer.normalizer import ConfigNormalizer\n",
    "normalized_values = model.get_sampled_configs()\n",
    "best_config_value = None\n",
    "best_config = {}\n",
    "best_out = sys.maxint\n",
    "# for config in list(itertools.product(*normalized_values)):\n",
    "for config in normalized_values:\n",
    "    out = model.predict(config)\n",
    "    if out < best_out:\n",
    "        best_out = out\n",
    "        best_config_value = config\n",
    "print best_out\n",
    "print best_config_value\n",
    "# model.normalizer.denormalize_config(best_config_value)\n",
    "i = 0\n",
    "for param in model.normalizer.get_params():\n",
    "    print ConfigNormalizer.denormalize_value(param, best_config_value[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.0, 5017.0, 768.0, 7.0]\n",
      "spark.sql.shuffle.partitions\n",
      "spark.executor.memory\n",
      "spark.driver.memory\n",
      "spark.executor.cores\n"
     ]
    }
   ],
   "source": [
    "#DEBUG Paragraph\n",
    "import sys\n",
    "from spark.discretizer.normalizer import ConfigNormalizer\n",
    "\n",
    "normalized_values = model.get_sampled_configs()\n",
    "best_config_value = None\n",
    "best_config = {}\n",
    "best_out = sys.maxint\n",
    "# for config in list(itertools.product(*normalized_values)):\n",
    "for config in normalized_values:\n",
    "    out = model.predict(config)\n",
    "    if out < best_out:\n",
    "        best_out = out\n",
    "        best_config_value = config\n",
    "\n",
    "denorm_best_config = model.normalizer.denormalize_config(best_config_value)\n",
    "print denorm_best_config\n",
    "for param in model.normalizer.get_params():\n",
    "    print param.get_name() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.spark_tuner import SparkTuner\n",
    "tuner = SparkTuner(config_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample_1 = {\n",
    "    \"spark.executor.memory\": 1024 * 5,\n",
    "    \"spark.sql.shuffle.partitions\": 100,\n",
    "    \"spark.executor.cores\": 4,\n",
    "    \"spark.driver.memory\": 1024\n",
    "}\n",
    "training_sample_2 = {\n",
    "    \"spark.executor.memory\": 1024 * 10,\n",
    "    \"spark.sql.shuffle.partitions\": 400,\n",
    "    \"spark.executor.cores\": 8,\n",
    "    \"spark.driver.memory\": 1024 * 3\n",
    "}\n",
    "tuner.add_sample_to_train_data(training_sample_1, 12)\n",
    "tuner.add_sample_to_train_data(training_sample_2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.memory': 768.0,\n",
       " 'spark.executor.cores': 7.0,\n",
       " 'spark.executor.memory': 5017.0,\n",
       " 'spark.sql.shuffle.partitions': 60.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.get_next_best_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.config.config_set import UniversalConfigSet\n",
    "from spark.model.gaussian_model import GaussianModel\n",
    "from spark.model.training_data import TrainingData\n",
    "from spark.config.config import Config\n",
    "from spark.discretizer.normalizer import ConfigNormalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TrainingData()\n",
    "config_set = UniversalConfigSet(4, 26544)\n",
    "model = GaussianModel(config_set, training_data)\n",
    "training_sample_1 = {\n",
    "    \"spark.executor.memory\": 11945,\n",
    "    \"spark.sql.shuffle.partitions\": 200,\n",
    "    \"spark.executor.cores\": 2,\n",
    "    \"spark.driver.memory\": 1024 * 4\n",
    "}\n",
    "training_sample_2 = {\n",
    "    \"spark.executor.memory\": 5972,\n",
    "    \"spark.sql.shuffle.partitions\": 300,\n",
    "    \"spark.executor.cores\": 1,\n",
    "    \"spark.driver.memory\": 1024 * 2\n",
    "}\n",
    "training_sample_3 = {\n",
    "    \"spark.executor.memory\": 11945,\n",
    "    \"spark.sql.shuffle.partitions\": 460,\n",
    "    \"spark.executor.cores\": 2,\n",
    "    \"spark.driver.memory\": 1024 * 4\n",
    "}\n",
    "training_sample_4 = {\n",
    "    \"spark.executor.memory\": 10068,\n",
    "    \"spark.sql.shuffle.partitions\": 1660,\n",
    "    \"spark.executor.cores\": 1,\n",
    "    \"spark.driver.memory\": 1024\n",
    "}\n",
    "model.add_sample_to_train_data(training_sample_1, 131)\n",
    "model.add_sample_to_train_data(training_sample_2, 143)\n",
    "model.add_sample_to_train_data(training_sample_3, 155)\n",
    "model.add_sample_to_train_data(training_sample_4, 343)\n",
    "model.train()\n",
    "config = Config(4, 26544)\n",
    "params = config_set.get_params()\n",
    "# for param in params:\n",
    "#     if param.get_name() == 'spark.executor.memory':\n",
    "#         config.add_param(param, 10068)\n",
    "#     elif param.get_name() == 'spark.sql.shuffle.partitions':\n",
    "#         config.add_param(param, 1660)\n",
    "#     elif param.get_name() == 'spark.executor.cores':\n",
    "#         config.add_param(param, 1)\n",
    "#     elif param.get_name() == 'spark.driver.memory':\n",
    "#         config.add_param(param, 1024)\n",
    "for param in params:\n",
    "    if param.get_name() == 'spark.executor.memory':\n",
    "        config.add_param(param, 10068)\n",
    "    elif param.get_name() == 'spark.sql.shuffle.partitions':\n",
    "        config.add_param(param, 1660)\n",
    "    elif param.get_name() == 'spark.executor.cores':\n",
    "        config.add_param(param, 1)\n",
    "    elif param.get_name() == 'spark.driver.memory':\n",
    "        config.add_param(param, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8461538461538461, 0.23529411764705882, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([147.39096822])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print config.get_all_param_names()\n",
    "# print config.get_all_param_values()\n",
    "norm_value = model.normalizer.normalize_config(config.get_all_param_values())\n",
    "print norm_value\n",
    "# print model.normalizer.denormalize_config(norm_value)\n",
    "model.predict(norm_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions\n",
      "1660\n",
      "10\n",
      "1960\n",
      "0.153846153846\n",
      "310.0\n",
      ">>>>>>>>\n",
      "spark.executor.memory\n",
      "10068\n",
      "5972\n",
      "23380\n",
      "0.764705882353\n",
      "19284.0\n",
      ">>>>>>>>\n",
      "spark.driver.memory\n",
      "1024\n",
      "1024\n",
      "6400\n",
      "1.0\n",
      "6400.0\n",
      ">>>>>>>>\n",
      "spark.executor.cores\n",
      "1\n",
      "1\n",
      "4\n",
      "1.0\n",
      "4.0\n",
      ">>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for param in model.normalizer.get_params():\n",
    "    print param.get_name()\n",
    "    print config.get_all_param_values()[i]\n",
    "    print param.get_domain().get_min()\n",
    "    print param.get_domain().get_max()    \n",
    "    a = ConfigNormalizer.normalize_value(param, config.get_all_param_values()[i])\n",
    "    print a\n",
    "    print ConfigNormalizer.denormalize_value(param, a)\n",
    "    i = i + 1\n",
    "    print \">>>>>>>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461538461538461"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1660 - 10) / float(1960 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
