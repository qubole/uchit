{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Combiner.\n",
    "\n",
    "```python\n",
    "from spark.combiner.combiner import Combiner\n",
    "combiner = Combiner(4, 26544)\n",
    "```\n",
    "\n",
    "- **4** is the **number of cores** per worker node.\n",
    "- **26544** is the **total memory** per worker node in MB. **90%** of the total memory are allocated for executors. If total memory allocated for executor is pre-decided let's say X MB, then set that value to X/0.9 MB ( as 90% of total memory is allocated to executor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.combiner.combiner import Combiner\n",
    "combiner = Combiner(4, 26544)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Training Data\n",
    "```python\n",
    "training_data_1 = {\n",
    "        \"spark.executor.memory\": 11945,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 10,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 0\n",
    "    }\n",
    "runtime_in_sec = 248\n",
    "combiner.add_training_data(training_data_1, runtime_in_sec)\n",
    "```\n",
    "Add the training data as a dictionary of config name and it's corresponding value.\n",
    "</br>\n",
    "Note for Boolean config, instead of *True* and *False* 0 or 1 needs to be used.\n",
    "</br>\n",
    "These are the only configs currently being modeled out of box:\n",
    "- spark.executor.memory\n",
    "- spark.sql.shuffle.partitions\n",
    "- spark.executor.cores\n",
    "- spark.driver.memory\n",
    "- spark.sql.autoBroadcastJoinThreshold\n",
    "- spark.sql.statistics.fallBackToHdfs\n",
    "</br>\n",
    "For adding more configs, you can add them to ConfigSet manually, which is out of scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = {\n",
    "        \"spark.executor.memory\": 11945,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 10,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 0\n",
    "    }\n",
    "runtime_in_sec = 248\n",
    "combiner.add_training_data(training_data_1, runtime_in_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute best config for next run\n",
    "```python\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config\n",
    "```\n",
    "Compute the best config for next run. Then run the job with the suggested configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark.executor.cores': 4, 'spark.driver.memory': 2048, 'spark.sql.statistics.fallBackToHdfs': 1, 'spark.sql.autoBroadcastJoinThreshold': 100, 'spark.executor.memory': 23889, 'spark.sql.shuffle.partitions': 200}\n"
     ]
    }
   ],
   "source": [
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add new training data from Step 3 and repeat the process `n` times\n",
    "\n",
    "* Run with suggested configs in Step 3 and based upon this run add new training data to model.\n",
    "* Get the new best config to run the job again.\n",
    "* Repeat this process for predefined `n` number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('spark.sql.shuffle.partitions', 1730.0), ('spark.executor.memory', 23876.0), ('spark.driver.memory', 5632.0), ('spark.executor.cores', 4.0), ('spark.sql.autoBroadcastJoinThreshold', 85.0), ('spark.sql.statistics.fallBackToHdfs', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "training_data_2 = {\n",
    "        \"spark.executor.memory\": 23889,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 4,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 100,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "runtime_in_sec = 92\n",
    "combiner.add_training_data(training_data_2, runtime_in_sec)\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('spark.sql.shuffle.partitions', 1930.0), ('spark.executor.memory', 5972.0), ('spark.driver.memory', 4864.0), ('spark.executor.cores', 1.0), ('spark.sql.autoBroadcastJoinThreshold', 95.0), ('spark.sql.statistics.fallBackToHdfs', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "training_data_3 = {\n",
    "        \"spark.executor.memory\": 11940,\n",
    "        \"spark.sql.shuffle.partitions\": 460,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 2304,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 30,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "runtime_in_sec = 105\n",
    "combiner.add_training_data(training_data_3, runtime_in_sec)\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('spark.sql.shuffle.partitions', 1750.0), ('spark.executor.memory', 23503.0), ('spark.driver.memory', 5888.0), ('spark.executor.cores', 4.0), ('spark.sql.autoBroadcastJoinThreshold', 30.0), ('spark.sql.statistics.fallBackToHdfs', 0.0)])\n"
     ]
    }
   ],
   "source": [
    "training_data_4 = {\n",
    "        \"spark.executor.memory\": 5972,\n",
    "        \"spark.sql.shuffle.partitions\": 1930,\n",
    "        \"spark.executor.cores\": 1,\n",
    "        \"spark.driver.memory\": 4864,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 95,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "runtime_in_sec = 121\n",
    "combiner.add_training_data(training_data_4, runtime_in_sec)\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5:  Choose the Config with best runtimes.\n",
    "In our example best config is:\n",
    "```python\n",
    "training_data_2 = {\n",
    "        \"spark.executor.memory\": 23889,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 4,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 100,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "```\n",
    "# Best Config gave 2.7X improvement over intial config.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
