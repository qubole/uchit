{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Combiner.\n",
    "\n",
    "```python\n",
    "from spark.combiner.combiner import Combiner\n",
    "combiner = Combiner(4, 26544)\n",
    "```\n",
    "\n",
    "- **4** is the **number of cores** per worker node.\n",
    "- **26544** is the **total memory** per worker node in MB. **90%** of the total memory are allocated for executors. If total memory allocated for executor is pre-decided let's say X MB, then set that value to X/0.9 MB ( as 90% of total memory is allocated to executor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.combiner.combiner import Combiner\n",
    "combiner = Combiner(4, 26544)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Add Training Data\n",
    "```python\n",
    "training_data_1 = {\n",
    "        \"spark.executor.memory\": 11945,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 10,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 0\n",
    "    }\n",
    "runtime_in_sec = 71\n",
    "combiner.add_training_data(training_data_1, runtime_in_sec)\n",
    "```\n",
    "Add the training data as a dictionary of config name and it's corresponding value.\n",
    "</br>\n",
    "Note for Boolean config, instead of *True* and *False* 0 or 1 needs to be used.\n",
    "</br>\n",
    "These are the only configs currently being modeled out of box:\n",
    "- spark.executor.memory\n",
    "- spark.sql.shuffle.partitions\n",
    "- spark.executor.cores\n",
    "- spark.driver.memory\n",
    "- spark.sql.autoBroadcastJoinThreshold\n",
    "- spark.sql.statistics.fallBackToHdfs\n",
    "For adding more configs, you can add them to ConfigSet manually, which is out of scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with TPC-DS Q17\n",
    "\n",
    "\n",
    "|conf|value|\n",
    "|:-|:-|\n",
    "|spark.driver.memory|2g|\n",
    "|spark.executor.cores|3|\n",
    "|spark.executor.memory|2g|\n",
    "|spark.sql.shuffle.partitions|400|\n",
    "\n",
    "\n",
    "<h5 align=\"center\">Timing for the query: q17 - 806750 </h5>\n",
    "\n",
    "================================================================================================================================================\n",
    "<br>\n",
    "\n",
    "\n",
    "|conf|value|\n",
    "|:-|:-|\n",
    "|spark.driver.memory|4g|\n",
    "|spark.executor.cores|8|\n",
    "|spark.executor.memory|5g|\n",
    "|spark.sql.shuffle.partitions|600|\n",
    "\n",
    "\n",
    "<h4 align=\"center\">Timing for the query: q17 - 1191319 </h4> \n",
    "\n",
    "================================================================================================================================================\n",
    "<br>\n",
    "\n",
    "\n",
    "|conf|value|\n",
    "|:-|:-|\n",
    "|spark.driver.memory|1g|\n",
    "|spark.executor.cores|2|\n",
    "|spark.executor.memory|2g|\n",
    "|spark.sql.shuffle.partitions|100|\n",
    "\n",
    "\n",
    "<h4 align=\"center\">Timing for the query: q17 - 1138390 </h4> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_1 = {\n",
    "        \"spark.executor.memory\": 11945,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 10,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 0\n",
    "    }\n",
    "runtime_in_sec = 248\n",
    "combiner.add_training_data(training_data_1, runtime_in_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spark.executor.cores': 4, 'spark.driver.memory': 2048, 'spark.sql.statistics.fallBackToHdfs': 1, 'spark.sql.autoBroadcastJoinThreshold': 100, 'spark.executor.memory': 23889, 'spark.sql.shuffle.partitions': 200}\n"
     ]
    }
   ],
   "source": [
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('spark.sql.shuffle.partitions', 460.0), ('spark.executor.memory', 11940.0), ('spark.driver.memory', 2304.0), ('spark.executor.cores', 2.0), ('spark.sql.autoBroadcastJoinThreshold', 30.0), ('spark.sql.statistics.fallBackToHdfs', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "training_data_2 = {\n",
    "        \"spark.executor.memory\": 23889,\n",
    "        \"spark.sql.shuffle.partitions\": 200,\n",
    "        \"spark.executor.cores\": 4,\n",
    "        \"spark.driver.memory\": 1024 * 2,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 100,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "runtime_in_sec = 92\n",
    "combiner.add_training_data(training_data_2, runtime_in_sec)\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('spark.sql.shuffle.partitions', 460.0), ('spark.executor.memory', 11940.0), ('spark.driver.memory', 2304.0), ('spark.executor.cores', 2.0), ('spark.sql.autoBroadcastJoinThreshold', 30.0), ('spark.sql.statistics.fallBackToHdfs', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "training_data_3 = {\n",
    "        \"spark.executor.memory\": 11940,\n",
    "        \"spark.sql.shuffle.partitions\": 460,\n",
    "        \"spark.executor.cores\": 2,\n",
    "        \"spark.driver.memory\": 2304,\n",
    "        \"spark.sql.autoBroadcastJoinThreshold\": 30,\n",
    "        \"spark.sql.statistics.fallBackToHdfs\": 1\n",
    "    }\n",
    "runtime_in_sec = 105\n",
    "combiner.add_training_data(training_data_3, runtime_in_sec)\n",
    "best_config = combiner.get_best_config()\n",
    "print best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'theta': 7.230629701585487, 'beta': 412696.6304998827, 'alpha1': 998.0269377497508, 'gamma': 0.5985143784548778}\n"
     ]
    }
   ],
   "source": [
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
